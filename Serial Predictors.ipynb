{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Model imports\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common interface to get clean datasets\n",
    "\n",
    "def get_clean_dataset_0():\n",
    "    \"\"\"\n",
    "    This gets you a cleans dataset 0.\n",
    "    APPROVED.\n",
    "    \"\"\"\n",
    "    return pd.read_csv('data/0/data.csv')\n",
    "\n",
    "\n",
    "def get_clean_dataset_2():\n",
    "    \"\"\"\n",
    "    This gets you a cleans dataset 2.\n",
    "    APPROVED.\n",
    "    \"\"\"\n",
    "    df = read_dataset_2('data/2/data.csv')\n",
    "    return clean_dataset_2(df)\n",
    "\n",
    "\n",
    "def get_clean_dataset_3():\n",
    "    \"\"\"\n",
    "    This gets you a cleans dataset 3.\n",
    "    APPROVED.\n",
    "    \"\"\"\n",
    "    return clean_dataset_3()\n",
    "\n",
    "\n",
    "def get_clean_dataset_4():\n",
    "    \"\"\"\n",
    "    This gets you a cleans dataset 4.\n",
    "    APPROVED.\n",
    "    \"\"\"\n",
    "    df  = read_dataset_4('data/4/data.tsv')\n",
    "    return clean_dataset_4(df)\n",
    "\n",
    "\n",
    "def get_clean_dataset_5():\n",
    "    \"\"\"\n",
    "    This gets you a cleans dataset 5.\n",
    "    \"\"\"\n",
    "    return clean_dataset_5()\n",
    "\n",
    "\n",
    "def get_clean_dataset_6():\n",
    "    \"\"\"\n",
    "    This gets you a cleans dataset 6.\n",
    "    \"\"\"\n",
    "    return clean_dataset_6()\n",
    "\n",
    "def get_clean_dataset_7():\n",
    "    \"\"\"\n",
    "    This gets you a cleans dataset 7.\n",
    "    APPROVED.\n",
    "    \"\"\"\n",
    "    df1, df2 = read_dataset_7()\n",
    "    return clean_dataset_7(df1, df2)\n",
    "\n",
    "\n",
    "def get_clean_dataset_8():\n",
    "    \"\"\"\n",
    "    This gets you a cleans dataset 8.\n",
    "    APPROVED.\n",
    "    \"\"\"\n",
    "    df = read_dataset_8()\n",
    "    return clean_dataset_8(df)\n",
    "\n",
    "\n",
    "def get_clean_dataset_9():\n",
    "    \"\"\"\n",
    "    This gets you a cleans dataset 9.\n",
    "    APPROVED.\n",
    "    \"\"\"\n",
    "    df = read_dataset_9('data/9/data.csv')\n",
    "    return clean_dataset_9(df)\n",
    "\n",
    "\n",
    "\n",
    "# Helper functions to clean datasets from all the team!\n",
    "\n",
    "def read_dataset_2(path):\n",
    "    \"\"\"\n",
    "    This reads the dataset 2.\n",
    "    \"\"\"\n",
    "    #In this case path is 'data/2/data.csv' as there is only one csv\n",
    "    df = pd.read_csv(path)\n",
    "    return df.copy()\n",
    "\n",
    "def clean_dataset_2(df):\n",
    "    \"\"\"\n",
    "    This cleans dataset 2.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['id'] = df.squirrel_id.str.replace(\"_\", \"\").astype(int)\n",
    "    return df.drop('squirrel_id', axis=1)\n",
    "\n",
    "\n",
    "def clean_dataset_3(filepath='data/3/data.csv'):\n",
    "    df = pd.read_csv(filepath)\n",
    "    # print('datase has {} rows and {} cols'.format(df.shape[0], df.shape[1]))\n",
    "    # df.set_index('id', inplace=True)\n",
    "    return df\n",
    "\n",
    "def read_dataset_4(path):\n",
    "    \"\"\"\n",
    "    This reads the dataset 4\n",
    "    This \n",
    "    \"\"\"\n",
    "    #In this case path is 'data/4/data.tsv' as there is only one csv\n",
    "    df = pd.read_json(path)\n",
    "    return df.copy()\n",
    "\n",
    "def clean_dataset_4(df):\n",
    "    \"\"\"\n",
    "    This cleans dataset 4.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\t# Your cleaning code here!\n",
    "    df = df.reset_index()\n",
    "    df['id'] = df['index'].str.split('-').map(lambda p: p[0])\n",
    "    df = df.drop('index', axis = 1)\n",
    "    df['id'] = df.id.astype(int)\n",
    "    df = df[df['id'] <= 1595]\n",
    "    df = df.set_index('id')\n",
    "    df = df.sort_index()\n",
    "    for i in df:\n",
    "        df[i] = df[i].str.replace('%', '').astype(float)\n",
    "    df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "def clean_dataset_5(filepath='data/5/data.csv'):\n",
    "    df = pd.read_csv(filepath)\n",
    "    # print('dataset has {} rows and {} cols'.format(df.shape[0], df.shape[1]))\n",
    "    #df.set_index('id', inplace=True)\n",
    "    df.index.name= 'id'\n",
    "    df = df.drop(['is_this_the_id_col', 'or_maybe_this_is_the_id_col'], axis=1)\n",
    "    df['violent_crimes_per_pop'] = df['Percentviolent_crimes_per_pop']\n",
    "    return df.drop('Percentviolent_crimes_per_pop', axis=1)\n",
    "\n",
    "def clean_dataset_6(filepath='data/6/data.html'):\n",
    "    # the file suffix is not relevant\n",
    "    \n",
    "    # fix decimal symbol; sep = '|'\n",
    "    df = pd.read_csv('data/6/data.html', sep='|', decimal=',')\n",
    "    # clean last row\n",
    "    df = df.iloc[:-1,:]\n",
    "    # print('dataset has {} rows and {} cols'.format(df.shape[0], df.shape[1]))\n",
    "    df.set_index('id', inplace=True)\n",
    "    return df\n",
    "\n",
    "def read_dataset_7():\n",
    "    \"\"\"\n",
    "    Reads and returns both datasets 7 as a tuple.\n",
    "    \"\"\"\n",
    "    df1 = pd.read_csv('data/7/data-1.csv')\n",
    "    df2 = pd.read_csv('data/7/data-2.csv')\n",
    "    \n",
    "    return df1, df2\n",
    "\n",
    "def clean_dataset_7(df1, df2):\n",
    "    clean_df1 = df1.set_index('id')\n",
    "    clean_df2 = df2.set_index('id')\n",
    "    \n",
    "    def _clean_float_columns(some_df):\n",
    "        \"\"\"\n",
    "        This assumes all the columns are floats!\n",
    "        \"\"\"\n",
    "        # Treat single whitespace as null values\n",
    "        float_cleaned_df = some_df.replace(' ', np.nan)\n",
    "        \n",
    "        float_columns = [c for c in float_cleaned_df.columns]\n",
    "\n",
    "        for c in float_columns:\n",
    "            float_cleaned_df[c] = float_cleaned_df[c].astype(float)\n",
    "        \n",
    "        return float_cleaned_df\n",
    "    \n",
    "    clean_df1 = _clean_float_columns(clean_df1)\n",
    "    clean_df2 = _clean_float_columns(clean_df2)\n",
    "    \n",
    "    clean_df1 = clean_df1.dropna(how='all')\n",
    "    clean_df2 = clean_df2.dropna(how='all')\n",
    "    \n",
    "    return pd.concat([clean_df1, clean_df2])\n",
    "\n",
    "\n",
    "def read_dataset_8():\n",
    "    #this loads dataset 8\n",
    "    last = 1595\n",
    "    dirty8 = load_partial_dataset_8(str(0))\n",
    "    dirty8['id'] = 0\n",
    "    for x in range(1, last):\n",
    "        dirty8_x = load_partial_dataset_8(str(x))\n",
    "        dirty8_x['id'] = x\n",
    "        dirty8 = dirty8.append(dirty8_x)\n",
    "    return dirty8\n",
    "\n",
    "def clean_dataset_8(df):\n",
    "    #This cleans dataset 8\n",
    "    return df\n",
    "\n",
    "def load_partial_dataset_8(index):\n",
    "    #this loads dataset 8\n",
    "    filename = 'data/8/' + index +'.csv'\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    df = pd.DataFrame(data, index=[index])\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_to_float(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def clean_dataset_9(df):\n",
    "    cleanDF = df[['pct_kids2_par', 'num_illeg', 'pct_w_inv_inc',\n",
    "       'black_per_cap', 'race_pct_white', 'racepctblack', 'own_occ_low_quart',\n",
    "       'hisp_per_cap', 'violent_crimes_per_pop']]\n",
    "    cleanDF['pct_illeg'] = df['pct_illeg'].apply(convert_to_float)\n",
    "    cleanDF['id'] = cleanDF.index\n",
    "    # cleanDF['pct_illeg'] = df['pct_illeg'].astype(float)\n",
    "    # cleanDF = cleanDF.dropna()\n",
    "    return cleanDF\n",
    "    \n",
    "def read_dataset_9(file):\n",
    "    return pd.read_csv(file, encoding='iso-8859-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreparedDataset(object):\n",
    "    \"\"\"\n",
    "    Represents a prepared dataset based on all available cleaning functions.\n",
    "    The dataset itself is lazily evaluated via the `df` property.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._df = None\n",
    "    \n",
    "    # Common interface to get clean datasets\n",
    "    \n",
    "    def get_clean_dataset_0(self):\n",
    "        \"\"\"\n",
    "        This gets you a cleans dataset 0.\n",
    "        APPROVED.\n",
    "        \"\"\"\n",
    "        return pd.read_csv('data/0/data.csv')\n",
    "\n",
    "    def get_clean_dataset_2(self):\n",
    "        \"\"\"\n",
    "        This gets you a cleans dataset 2.\n",
    "        APPROVED.\n",
    "        \"\"\"\n",
    "        df = read_dataset_2('data/2/data.csv')\n",
    "        return clean_dataset_2(df)\n",
    "    \n",
    "    def get_clean_dataset_3(self):\n",
    "        \"\"\"\n",
    "        This gets you a cleans dataset 3.\n",
    "        APPROVED.\n",
    "        \"\"\"\n",
    "        return clean_dataset_3()\n",
    "\n",
    "    def get_clean_dataset_4(self):\n",
    "        \"\"\"\n",
    "        This gets you a cleans dataset 4.\n",
    "        APPROVED.\n",
    "        \"\"\"\n",
    "        df  = read_dataset_4('data/4/data.tsv')\n",
    "        return clean_dataset_4(df)\n",
    "\n",
    "    def get_clean_dataset_5(self):\n",
    "        \"\"\"\n",
    "        This gets you a cleans dataset 5.\n",
    "        APPROVED.\n",
    "        \"\"\"\n",
    "        return clean_dataset_5()\n",
    "\n",
    "    def get_clean_dataset_6(self):\n",
    "        \"\"\"\n",
    "        This gets you a cleans dataset 6.\n",
    "        APPROVED.\n",
    "        \"\"\"\n",
    "        return clean_dataset_6()\n",
    "\n",
    "    def get_clean_dataset_7(self):\n",
    "        \"\"\"\n",
    "        This gets you a cleans dataset 7.\n",
    "        APPROVED.\n",
    "        \"\"\"\n",
    "        df1, df2 = read_dataset_7()\n",
    "        return clean_dataset_7(df1, df2)\n",
    "\n",
    "    def get_clean_dataset_8(self):\n",
    "        \"\"\"\n",
    "        This gets you a cleans dataset 8.\n",
    "        APPROVED.\n",
    "        \"\"\"\n",
    "        df = read_dataset_8()\n",
    "        return clean_dataset_8(df)\n",
    "\n",
    "    def get_clean_dataset_9(self):\n",
    "        \"\"\"\n",
    "        This gets you a cleans dataset 9.\n",
    "        APPROVED.\n",
    "        \"\"\"\n",
    "        df = read_dataset_9('data/9/data.csv')\n",
    "        return clean_dataset_9(df)\n",
    "\n",
    "    def _pre_merge(self):\n",
    "        get_clean_datasets = [f for f in dir(self) if f.startswith('get_clean_dataset_')]\n",
    "        get_clean_datasets_functions = [getattr(self, func_name) for func_name in get_clean_datasets]\n",
    "        \n",
    "        logging.info('[serial-predictors] collecting cleaned datasets {}'.format(\n",
    "            [func_name.split()[-1] for func_name in get_clean_datasets])\n",
    "        )\n",
    "        \n",
    "        datasets = [fn() for fn in get_clean_datasets_functions]\n",
    "        \n",
    "        # reset any indexes\n",
    "        datasets = [\n",
    "            d.reset_index().set_index('id')\n",
    "            for d in datasets\n",
    "        ]\n",
    "        \n",
    "        # drop the target column except on the first one\n",
    "        map(lambda d: d.drop(TARGET, axis=1, inplace=True), datasets[1:])\n",
    "        \n",
    "        self.datasets = datasets\n",
    "    \n",
    "    def _merge(self):\n",
    "        from functools import reduce\n",
    "        self.merged_dataset = self.datasets[0]\n",
    "        \n",
    "        for d in self.datasets[1:]:\n",
    "            self.merged_dataset.join(d)\n",
    "    \n",
    "    def _post_merge(self):\n",
    "        self._df = self.merged_dataset.reset_index().dropna()\n",
    "    \n",
    "    def _prepare_dataset(self):\n",
    "        self._pre_merge()\n",
    "        self._merge()\n",
    "        self._post_merge()\n",
    "    \n",
    "    @property\n",
    "    def df(self):\n",
    "        if not self._df:\n",
    "            self._prepare_dataset()\n",
    "        return self._df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:220: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "    # Merges ALL THE DATASETS\n",
    "\n",
    "df0 = get_clean_dataset_0()\n",
    "df2 = get_clean_dataset_2()\n",
    "df3 = get_clean_dataset_3()\n",
    "df4 = get_clean_dataset_4()\n",
    "df5 = get_clean_dataset_5()\n",
    "df6 = get_clean_dataset_6()\n",
    "df7 = get_clean_dataset_7()\n",
    "df8 = get_clean_dataset_8()\n",
    "df9 = get_clean_dataset_9()\n",
    "\n",
    "df0 = df0.set_index('id')\n",
    "df2 = df2.set_index('id')\n",
    "df3 = df3.set_index('id')\n",
    "# df4 = df4.set_index('id') --> already has index!\n",
    "# df5 = df5.set_index('id') --> already has index!\n",
    "# df6 = df6.set_index('id') --> already has index!\n",
    "# df7 = df7.set_index('id') --> already has index!\n",
    "df8 = df8.set_index('id')\n",
    "df9 = df9.set_index('id')\n",
    "\n",
    "df2 = df2.drop(TARGET, axis=1)\n",
    "df3 = df3.drop(TARGET, axis=1)\n",
    "df4 = df4.drop(TARGET, axis=1)\n",
    "df5 = df5.drop(TARGET, axis=1)\n",
    "df6 = df6.drop(TARGET, axis=1)\n",
    "df7 = df7.drop(TARGET, axis=1)\n",
    "df8 = df8.drop(TARGET, axis=1)\n",
    "df9 = df9.drop(TARGET, axis=1)\n",
    "\n",
    "\n",
    "df = df0.join(df2).join(df3).join(df4).join(df5).join(df7).join(df8).join(df9)\n",
    "\n",
    "df = df.reset_index()\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:220: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "columns overlap but no suffix specified: Index(['index', 'violent_crimes_per_pop'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-a89372a2384d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreparedDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-1927b43fd2bb>\u001b[0m in \u001b[0;36mdf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-1927b43fd2bb>\u001b[0m in \u001b[0;36m_prepare_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pre_merge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_merge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-1927b43fd2bb>\u001b[0m in \u001b[0;36m_merge\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerged_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_post_merge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   4667\u001b[0m         \u001b[0;31m# For SparseDataFrame's benefit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4668\u001b[0m         return self._join_compat(other, on=on, how=how, lsuffix=lsuffix,\n\u001b[0;32m-> 4669\u001b[0;31m                                  rsuffix=rsuffix, sort=sort)\n\u001b[0m\u001b[1;32m   4670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4671\u001b[0m     def _join_compat(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_join_compat\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   4682\u001b[0m             return merge(self, other, left_on=on, how=how,\n\u001b[1;32m   4683\u001b[0m                          \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4684\u001b[0;31m                          suffixes=(lsuffix, rsuffix), sort=sort)\n\u001b[0m\u001b[1;32m   4685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4686\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator)\u001b[0m\n\u001b[1;32m     52\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                          copy=copy, indicator=indicator)\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         llabels, rlabels = items_overlap_with_suffix(ldata.items, lsuf,\n\u001b[0;32m--> 575\u001b[0;31m                                                      rdata.items, rsuf)\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mlindexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mleft_indexer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mitems_overlap_with_suffix\u001b[0;34m(left, lsuffix, right, rsuffix)\u001b[0m\n\u001b[1;32m   4699\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlsuffix\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4700\u001b[0m             raise ValueError('columns overlap but no suffix specified: %s' %\n\u001b[0;32m-> 4701\u001b[0;31m                              to_rename)\n\u001b[0m\u001b[1;32m   4702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4703\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mlrenamer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: columns overlap but no suffix specified: Index(['index', 'violent_crimes_per_pop'], dtype='object')"
     ]
    }
   ],
   "source": [
    "df = PreparedDataset().df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL FITTING AND SUBMISSION UTILS\n",
    "\n",
    "def hyper_parameters_tuning_RFRegressor(X, y):\n",
    "    # Define the parameter space\n",
    "    parameter_space_dist = {\"max_depth\": range(1,10), \"n_estimators\": range(1,5)}\n",
    "    \n",
    "    # Choose the classifier\n",
    "    rf = RandomForestRegressor()\n",
    "    \n",
    "    # Select grid search with cross validation\n",
    "    random_search = RandomizedSearchCV(rf, parameter_space_dist, n_iter=5, n_jobs=-1)\n",
    "\n",
    "    # Fit the model based on the train datasets to the random search for the hyper parameters tunning\n",
    "    random_search.fit(X, y)\n",
    "\n",
    "    # Redefine the model with best parameters found in the hyper parameters tunning\n",
    "    rf = RandomForestRegressor(max_depth=random_search.best_params_['max_depth'], n_jobs=-1\n",
    "                             , n_estimators=random_search.best_params_['n_estimators'])\n",
    "\n",
    "    # Fit the model\n",
    "    rf.fit(X,y)\n",
    "\n",
    "    # Cross validation\n",
    "    cross_val_score = evaluate(rf, X, y, scoring='r2')\n",
    "    print('Optimized RF score is {}.'.format(cross_val_score))\n",
    "    return rf\n",
    "\n",
    "\n",
    "def hyper_parameters_tuning_GBoostRegressor(X, y):\n",
    "    # Define the parameter space\n",
    "    # parameter_space_dist = {\"max_depth\": range(1,5), \"n_estimators\": range(1,50)}\n",
    "    \n",
    "    # Choose the classifier\n",
    "    rf = GradientBoostingRegressor()\n",
    "    \n",
    "    # Select grid search with cross validation\n",
    "    #random_search = RandomizedSearchCV(rf, parameter_space_dist, n_iter=5, n_jobs=-1)\n",
    "\n",
    "    # Fit the model based on the train datasets to the random search for the hyper parameters tunning\n",
    "    #random_search.fit(X, y)\n",
    "\n",
    "    # Redefine the model with best parameters found in the hyper parameters tunning\n",
    "    #rf = RandomForestRegressor(max_depth=random_search.best_params_['max_depth'], n_jobs=-1\n",
    "                            # , n_estimators=random_search.best_params_['n_estimators'])\n",
    "\n",
    "    \n",
    "    # Cross validation\n",
    "    cross_val_score = evaluate(rf, X, y, scoring='r2')\n",
    "    print('Optimized RF score is {}.'.format(cross_val_score))\n",
    "    \n",
    "    # Fit the model\n",
    "    rf.fit(X,y)\n",
    "    \n",
    "    return rf\n",
    "\n",
    "def hyper_parameters_tuning_GBoostRegressor_optimized(X, y):\n",
    "    # Define the parameter space\n",
    "    parameter_space_dist = {'learning_rate': [0.1, 0.05, 0.02, 0.01],\n",
    "              'max_depth': [4, 6, 8],\n",
    "              'min_samples_leaf': [20, 50,100,150],\n",
    "              'max_features': [1.0, 0.3, 0.1] }\n",
    "  \n",
    "    # Choose the classifier\n",
    "    rf = GradientBoostingRegressor()\n",
    "    \n",
    "    # Select grid search with cross validation\n",
    "    random_search = RandomizedSearchCV(rf, parameter_space_dist, n_iter=100, n_jobs=-1)\n",
    "\n",
    "    # Fit the model based on the train datasets to the random search for the hyper parameters tunning\n",
    "    random_search.fit(X, y)\n",
    "\n",
    "    # Redefine the model with best parameters found in the hyper parameters tunning\n",
    "    rf = GradientBoostingRegressor(max_depth=random_search.best_params_['max_depth'],\n",
    "                            learning_rate=random_search.best_params_['learning_rate'],\n",
    "                            min_samples_leaf=random_search.best_params_['min_samples_leaf'],\n",
    "                            max_features=random_search.best_params_['max_features'])\n",
    "\n",
    "    # Cross validation\n",
    "    cross_val_score = evaluate(rf, X, y, scoring='r2')\n",
    "    print('Optimized RF score is {}.'.format(cross_val_score))\n",
    "    \n",
    "     # Fit the model\n",
    "    rf.fit(X,y)\n",
    "    return rf\n",
    "\n",
    "\n",
    "def evaluate(estimator, X, y, scoring='r2'):\n",
    "    return cross_val_score(estimator, \n",
    "                    X, \n",
    "                    y, \n",
    "                    scoring=scoring, \n",
    "                    cv=None, \n",
    "                    n_jobs=-1).mean()\n",
    "\n",
    "def feature_importance():\n",
    "    \"\"\"\n",
    "    See the feature importances\n",
    "    \"\"\"\n",
    "    # don't forget to import %matplotlib inline\n",
    "    rf = hyper_parameters_tuning_GBoostRegressor(X_train, y_train)\n",
    "    my_importances = pd.Series(dict(zip(X_train.columns, rf.feature_importances_))).sort_values()\n",
    "    # my_importances.plot(kind='barh',figsize=(16,10))\n",
    "    return my_importances\n",
    "\n",
    "\n",
    "def prepare_and_generate_submission(model, features):\n",
    "    X_test_original = pd.read_csv('X_test.csv')\n",
    "    X_test = X_test_original[features]\n",
    "    predictions = model.predict(X_test)\n",
    "    submission_df = X_test.drop([c for c in X_test.columns if c!= 'id'], axis=1)\n",
    "    submission_df['violent_crimes_per_pop'] = predictions\n",
    "    \n",
    "    submission_df.to_csv('submission_serial_predictors_5.csv', index=False)\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'violent_crimes_per_pop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:220: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pct_immig_rec5</th>\n",
       "      <th>med_rent_pct_hous_inc</th>\n",
       "      <th>pct_immig_rec10</th>\n",
       "      <th>med_own_cost_pct_inc_no_mtg</th>\n",
       "      <th>pct_w_wage</th>\n",
       "      <th>age_pct12t29</th>\n",
       "      <th>pct_fam2_par</th>\n",
       "      <th>med_yr_hous_built</th>\n",
       "      <th>pct_foreign_born</th>\n",
       "      <th>num_under_pov</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pct_immig_rec5  med_rent_pct_hous_inc  pct_immig_rec10  \\\n",
       "id                                                           \n",
       "0             0.38                   0.37             0.41   \n",
       "1             0.24                   0.60             0.28   \n",
       "2             0.50                   0.53             0.57   \n",
       "3             0.53                   0.71             0.69   \n",
       "4             0.59                   0.41             0.61   \n",
       "\n",
       "    med_own_cost_pct_inc_no_mtg  pct_w_wage  age_pct12t29  pct_fam2_par  \\\n",
       "id                                                                        \n",
       "0                          0.61        0.34          0.42          0.03   \n",
       "1                          0.56        0.74          0.36          0.94   \n",
       "2                          0.46        0.51          0.53          0.18   \n",
       "3                          0.19        0.55          0.53          0.59   \n",
       "4                          0.36        0.50          0.56          0.54   \n",
       "\n",
       "    med_yr_hous_built  pct_foreign_born  num_under_pov  \n",
       "id                                                      \n",
       "0                0.29              0.01           0.02  \n",
       "1                0.52              0.16           0.00  \n",
       "2                0.46              0.04           1.00  \n",
       "3                0.31              1.00           0.05  \n",
       "4                0.25              0.24           0.23  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merges ALL THE DATASETS\n",
    "\n",
    "df0 = get_clean_dataset_0()\n",
    "df2 = get_clean_dataset_2()\n",
    "df3 = get_clean_dataset_3()\n",
    "df4 = get_clean_dataset_4()\n",
    "df5 = get_clean_dataset_5()\n",
    "df6 = get_clean_dataset_6()\n",
    "df7 = get_clean_dataset_7()\n",
    "df8 = get_clean_dataset_8()\n",
    "df9 = get_clean_dataset_9()\n",
    "\n",
    "df0 = df0.set_index('id')\n",
    "df2 = df2.set_index('id')\n",
    "df3 = df3.set_index('id')\n",
    "# df4 = df4.set_index('id') --> already has index!\n",
    "# df5 = df5.set_index('id') --> already has index!\n",
    "# df6 = df6.set_index('id') --> already has index!\n",
    "# df7 = df7.set_index('id') --> already has index!\n",
    "df8 = df8.set_index('id')\n",
    "df9 = df9.set_index('id')\n",
    "\n",
    "df2 = df2.drop(TARGET, axis=1)\n",
    "df3 = df3.drop(TARGET, axis=1)\n",
    "df4 = df4.drop(TARGET, axis=1)\n",
    "df5 = df5.drop(TARGET, axis=1)\n",
    "df6 = df6.drop(TARGET, axis=1)\n",
    "df7 = df7.drop(TARGET, axis=1)\n",
    "df8 = df8.drop(TARGET, axis=1)\n",
    "df9 = df9.drop(TARGET, axis=1)\n",
    "\n",
    "\n",
    "df = df0.join(df2).join(df3).join(df4).join(df5).join(df7).join(df8).join(df9)\n",
    "\n",
    "df = df.reset_index()\n",
    "df = df.dropna()\n",
    "\n",
    "\n",
    "df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized RF score is 0.6093342450056064.\n"
     ]
    }
   ],
   "source": [
    "# Generates X and y for training\n",
    "\n",
    "features = [c for c in df.columns if c != TARGET]\n",
    "top_features = feature_importance().reset_index()['index'][-30:].values\n",
    "features = top_features\n",
    "X_train = df[features]\n",
    "y_train = df[TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized RF score is 0.6170540353147497.\n"
     ]
    }
   ],
   "source": [
    "submission = prepare_and_generate_submission(\n",
    "    model=hyper_parameters_tuning_GBoostRegressor_optimized(X_train, y_train),\n",
    "    features=features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
