{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Model imports\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common interface to get clean datasets\n",
    "\n",
    "def get_clean_dataset_0():\n",
    "    \"\"\"\n",
    "    This gets you a cleans dataset 0.\n",
    "    \"\"\"\n",
    "    return pd.read_csv('data/0/data.csv')\n",
    "\n",
    "\n",
    "def get_clean_dataset_2():\n",
    "    \"\"\"\n",
    "    This gets you a cleans dataset 2.\n",
    "    \"\"\"\n",
    "    df = read_dataset_2('data/2/data.csv')\n",
    "    return clean_dataset_2(df)\n",
    "\n",
    "\n",
    "def get_clean_dataset_7():\n",
    "    \"\"\"\n",
    "    This gets you a cleans dataset 7.\n",
    "    \"\"\"\n",
    "    df1, df2 = read_dataset_7()\n",
    "    return clean_dataset_7(df1, df2)\n",
    "\n",
    "\n",
    "def get_clean_dataset_8():\n",
    "    \"\"\"\n",
    "    This gets you a cleans dataset 8.\n",
    "    \"\"\"\n",
    "    df = read_dataset_8()\n",
    "    return clean_dataset_8(df)\n",
    "\n",
    "\n",
    "def get_clean_dataset_9():\n",
    "    \"\"\"\n",
    "    This gets you a cleans dataset 9.\n",
    "    \"\"\"\n",
    "    df = read_dataset_9('data/9/data.csv')\n",
    "    return clean_dataset_9(df)\n",
    "\n",
    "\n",
    "\n",
    "# Helper functions to clean datasets from all the team!\n",
    "\n",
    "def read_dataset_2(path):\n",
    "    \"\"\"\n",
    "    This reads the dataset 2.\n",
    "    \"\"\"\n",
    "    #In this case path is 'data/2/data.csv' as there is only one csv\n",
    "    df = pd.read_csv(path)\n",
    "    return df.copy()\n",
    "\n",
    "def clean_dataset_2(df):\n",
    "    \"\"\"\n",
    "    This cleans dataset 2.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['id'] = df.squirrel_id.str.replace(\"_\", \"\").astype(int)\n",
    "    return df.drop('squirrel_id', axis=1)\n",
    "\n",
    "\n",
    "def read_dataset_7():\n",
    "    \"\"\"\n",
    "    Reads and returns both datasets 7 as a tuple.\n",
    "    \"\"\"\n",
    "    df1 = pd.read_csv('data/7/data-1.csv')\n",
    "    df2 = pd.read_csv('data/7/data-2.csv')\n",
    "    \n",
    "    return df1, df2\n",
    "\n",
    "def clean_dataset_7(df1, df2):\n",
    "    clean_df1 = df1.set_index('id')\n",
    "    clean_df2 = df2.set_index('id')\n",
    "    \n",
    "    def _clean_float_columns(some_df):\n",
    "        \"\"\"\n",
    "        This assumes all the columns are floats!\n",
    "        \"\"\"\n",
    "        # Treat single whitespace as null values\n",
    "        float_cleaned_df = some_df.replace(' ', np.nan)\n",
    "        \n",
    "        float_columns = [c for c in float_cleaned_df.columns]\n",
    "\n",
    "        for c in float_columns:\n",
    "            float_cleaned_df[c] = float_cleaned_df[c].astype(float)\n",
    "        \n",
    "        return float_cleaned_df\n",
    "    \n",
    "    clean_df1 = _clean_float_columns(clean_df1)\n",
    "    clean_df2 = _clean_float_columns(clean_df2)\n",
    "    \n",
    "    clean_df1 = clean_df1.dropna(how='all')\n",
    "    clean_df2 = clean_df2.dropna(how='all')\n",
    "    \n",
    "    return pd.concat([clean_df1, clean_df2])\n",
    "\n",
    "\n",
    "def read_dataset_8():\n",
    "    #this loads dataset 8\n",
    "    last = 1595\n",
    "    dirty8 = load_partial_dataset_8(str(0))\n",
    "    dirty8['id'] = 0\n",
    "    for x in range(1, last):\n",
    "        dirty8_x = load_partial_dataset_8(str(x))\n",
    "        dirty8_x['id'] = x\n",
    "        dirty8 = dirty8.append(dirty8_x)\n",
    "    return dirty8\n",
    "\n",
    "def clean_dataset_8(df):\n",
    "    #This cleans dataset 8\n",
    "    return df\n",
    "\n",
    "def load_partial_dataset_8(index):\n",
    "    #this loads dataset 8\n",
    "    filename = 'data/8/' + index +'.csv'\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    df = pd.DataFrame(data, index=[index])\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_to_float(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def clean_dataset_9(df):\n",
    "    cleanDF = df[['pct_kids2_par', 'num_illeg', 'pct_w_inv_inc',\n",
    "       'black_per_cap', 'race_pct_white', 'racepctblack', 'own_occ_low_quart',\n",
    "       'hisp_per_cap', 'violent_crimes_per_pop']]\n",
    "    cleanDF['pct_illeg'] = df['pct_illeg'].apply(convert_to_float)\n",
    "    cleanDF = cleanDF.dropna()\n",
    "    return cleanDF\n",
    "    \n",
    "def read_dataset_9(file):\n",
    "    return pd.read_csv(file, encoding='cp1254')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f71ee9103480>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf9\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_clean_dataset_9\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-a0fa1b2d1ff2>\u001b[0m in \u001b[0;36mget_clean_dataset_9\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0mgets\u001b[0m \u001b[0myou\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcleans\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;36m9.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \"\"\"\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_dataset_9\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/9/data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclean_dataset_9\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a0fa1b2d1ff2>\u001b[0m in \u001b[0;36mread_dataset_9\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_dataset_9\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cp1254'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:6260)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "df9 = get_clean_dataset_9()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL FITTING AND SUBMISSION UTILS\n",
    "\n",
    "def hyper_parameters_tuning_RFRegressor(X, y):\n",
    "    # Define the parameter space\n",
    "    parameter_space_dist = {\"max_depth\": range(1,10), \"n_estimators\": range(1,5)}\n",
    "    \n",
    "    # Choose the classifier\n",
    "    rf = RandomForestRegressor()\n",
    "    \n",
    "    # Select grid search with cross validation\n",
    "    random_search = RandomizedSearchCV(rf, parameter_space_dist, n_iter=5, n_jobs=-1)\n",
    "\n",
    "    # Fit the model based on the train datasets to the random search for the hyper parameters tunning\n",
    "    random_search.fit(X, y)\n",
    "\n",
    "    # Redefine the model with best parameters found in the hyper parameters tunning\n",
    "    rf = RandomForestRegressor(max_depth=random_search.best_params_['max_depth'], n_jobs=-1\n",
    "                             , n_estimators=random_search.best_params_['n_estimators'])\n",
    "\n",
    "    # Fit the model\n",
    "    rf.fit(X,y)\n",
    "\n",
    "    # Cross validation\n",
    "    cross_val_score = evaluate(rf, X, y, scoring='r2')\n",
    "    print('Optimized RF score is {}.'.format(cross_val_score))\n",
    "    return rf\n",
    "\n",
    "\n",
    "def evaluate(estimator, X, y, scoring='r2'):\n",
    "    return cross_val_score(estimator, \n",
    "                    X, \n",
    "                    y, \n",
    "                    scoring=scoring, \n",
    "                    cv=None, \n",
    "                    n_jobs=-1).mean()\n",
    "\n",
    "\n",
    "def prepare_and_generate_submission(model, features):\n",
    "    X_test_original = pd.read_csv('X_test.csv')\n",
    "    X_test = X_test_original.drop([c for c in X_test_original.columns if c not in features], axis=1)\n",
    "    predictions = model.predict(X_test)\n",
    "    submission_df = X_test.drop([c for c in X_test.columns if c!= 'id'], axis=1)\n",
    "    submission_df['violent_crimes_per_pop'] = predictions\n",
    "    \n",
    "    submission_df.to_csv('submission_serial_predictors_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_clean_dataset_0()\n",
    "\n",
    "\n",
    "# Generates X and y for training\n",
    "target = 'violent_crimes_per_pop'\n",
    "features = [c for c in df.columns if c != target]\n",
    "X_train = df[features]\n",
    "y_train = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_and_generate_submission(\n",
    "    hyper_parameters_tuning_RFRegressor(X_train, y_train),\n",
    "    features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
